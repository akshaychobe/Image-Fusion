{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9N2C3QEeX-c",
        "outputId": "fa834dac-b91a-4ae7-a34b-c4b386f98fa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "KpsTNTOystKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from skimage.io import imread, imshow, imsave\n",
        "from skimage.transform import resize\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score"
      ],
      "metadata": {
        "id": "x78JjSRSssPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "Wo1ylFAFt0R3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A dictionary mapping from shape names to numerical labels.\n",
        "shape2number = {'circle': 0, 'square': 1, 'triangle': 2, 'pentagon': 3}\n",
        "\n",
        "# Dataset Path\n",
        "dataset_path = '/content/drive/MyDrive/SF2/Imagefusion_Dataset-2'\n",
        "\n",
        "# Define the folders for each sensor\n",
        "img1_folder = os.path.join(dataset_path, 'img1')\n",
        "img2_folder = os.path.join(dataset_path, 'img2')\n",
        "img3_folder = os.path.join(dataset_path, 'img3')\n",
        "fused_folder = os.path.join(dataset_path, 'fused_dataset')\n",
        "\n",
        "\n",
        "# Create the fused folder if it doesn't exist\n",
        "if not os.path.exists(fused_folder):\n",
        "    os.makedirs(fused_folder)\n",
        "\n",
        "\n",
        "\n",
        "# Get a list of all image files in each folder\n",
        "img1_files = [f for f in os.listdir(img1_folder) if f.endswith('.png')]\n",
        "img2_files = [f for f in os.listdir(img2_folder) if f.endswith('.png')]\n",
        "img3_files = [f for f in os.listdir(img3_folder) if f.endswith('.png')]\n"
      ],
      "metadata": {
        "id": "77xk1--O0nAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Low-Level Fusion Methods"
      ],
      "metadata": {
        "id": "qJl-0Mn-uB0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def average_fusion(image1, image2, image3):\n",
        "    if len(image1.shape) == 3:\n",
        "        image1 = image1.mean(axis = 2)\n",
        "    if len(image2.shape) == 3:\n",
        "        image2 = image2.mean(axis = 2)\n",
        "    if len(image3.shape) == 3:\n",
        "        image3 = image3.mean(axis = 2)\n",
        "    return (image1 + image2 + image3) / 3\n",
        "\n",
        "def max_selection_fusion(image1, image2, image3):\n",
        "    if len(image1.shape) == 3:\n",
        "        image1 = image1.max(axis = 2)\n",
        "    if len(image2.shape) == 3:\n",
        "        image2 = image2.max(axis = 2)\n",
        "    if len(image3.shape) == 3:\n",
        "        image3 = image3.max(axis = 2)\n",
        "    return np.maximum(np.maximum(image1, image2), image3)\n",
        "\n",
        "def min_selection_fusion(image1, image2, image3):\n",
        "    if len(image1.shape) == 3:\n",
        "        image1 = image1.min(axis = 2)\n",
        "    if len(image2.shape) == 3:\n",
        "        image2 = image2.min(axis = 2)\n",
        "    if len(image3.shape) == 3:\n",
        "        image3 = image3.min(axis = 2)\n",
        "    return np.minimum(np.minimum(image1, image2), image3)\n",
        "\n",
        "fused_images= []\n",
        "\n",
        "# Apply Fusion to all images\n",
        "for i, (img1_file, img2_file, img3_file) in enumerate(zip(img1_files, img2_files, img3_files)):\n",
        "    image1 = imread(os.path.join(img1_folder, img1_file)).astype(float)\n",
        "    image2 = imread(os.path.join(img2_folder, img2_file)).astype(float)\n",
        "    image3 = imread(os.path.join(img3_folder, img3_file)).astype(float)\n",
        "\n",
        "    fused_average = average_fusion(image1, image2, image3)\n",
        "    fused_max = max_selection_fusion(image1, image2, image3)\n",
        "    fused_min = min_selection_fusion(image1, image2, image3)\n",
        "\n",
        "    # Store the fused average images in the list\n",
        "    fused_images.append(fused_average)\n",
        "\n",
        "    # Save the fused average images to the specified directory\n",
        "    fused_image_filename = os.path.join(fused_folder, f'{str(i).zfill(5)}.png')\n",
        "    imsave(fused_image_filename, fused_average.astype(np.uint8))\n",
        "\n",
        "\n",
        "    # Show Result\n",
        "    plt.figure(figsize=(10, 12))\n",
        "    plt.suptitle(f'Image No. {str(i+1).zfill(5)}', fontsize=16)\n",
        "\n",
        "    plt.subplot(3, 2, 1)\n",
        "    plt.title('Gradient')\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image1.astype(np.uint8))\n",
        "\n",
        "    plt.subplot(3, 2, 2)\n",
        "    plt.title('Noise')\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image2.astype(np.uint8), cmap=None)\n",
        "\n",
        "    plt.subplot(3, 2, 3)\n",
        "    plt.title('Spotlight')\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image3.astype(np.uint8), cmap=None)\n",
        "\n",
        "    plt.subplot(3, 2, 4)\n",
        "    plt.title('Average Fusion')\n",
        "    plt.axis('off')\n",
        "    plt.imshow(fused_average.astype(np.uint8), cmap=None)\n",
        "\n",
        "    plt.subplot(3, 2, 5)\n",
        "    plt.title('Max Selection Fusion')\n",
        "    plt.axis('off')\n",
        "    plt.imshow(fused_max.astype(np.uint8), cmap=None)\n",
        "\n",
        "    plt.subplot(3, 2, 6)\n",
        "    plt.title('Min Selection Fusion')\n",
        "    plt.axis('off')\n",
        "    plt.imshow(fused_min.astype(np.uint8), cmap=None)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "2A76ZImufC6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Classes for handelling individual sensor dataset and Image fusion dataset"
      ],
      "metadata": {
        "id": "UQ8CqbkLuLUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleSensorDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset class for handling images from a single sensor along with their labels.\n",
        "\n",
        "    Attributes:\n",
        "        img_dir (str): Directory containing the images.\n",
        "        label_dir (str): Directory containing the labels.\n",
        "        transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        image_files (list): List of image filenames.\n",
        "        labels (list): List of labels corresponding to the images.\n",
        "    \"\"\"\n",
        "    def __init__(self, img_dir, label_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Initialize the SingleSensorDataset.\n",
        "\n",
        "        Args:\n",
        "            img_dir (str): Directory containing the images.\n",
        "            label_dir (str): Directory containing the labels.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.img_dir = img_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = sorted(os.listdir(img_dir))\n",
        "        self.labels = [self.load_label(os.path.join(label_dir, f\"{os.path.splitext(img)[0]}.txt\")) for img in self.image_files]\n",
        "\n",
        "    def load_label(self, idx):\n",
        "        \"\"\"\n",
        "        Load the label for a given image.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the image.\n",
        "\n",
        "        Returns:\n",
        "            int: Label associated with the image.\n",
        "        \"\"\"\n",
        "        label_path = os.path.join(self.label_dir, f\"{idx}\")\n",
        "        with open(label_path, 'r') as file:\n",
        "            label = file.readline().strip()\n",
        "        return shape2number[label]\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Get the length of the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: Number of samples in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Get an item from the dataset.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the item.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing the image and its label.\n",
        "        \"\"\"\n",
        "        img_path = os.path.join(self.img_dir, self.image_files[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        img = Image.open(img_path)\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "class ImageFusionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset class for handling fused images from multiple sensors along with their labels.\n",
        "\n",
        "    Attributes:\n",
        "        img1_dir (str): Directory containing the first set of images.\n",
        "        img2_dir (str): Directory containing the second set of images.\n",
        "        img3_dir (str): Directory containing the third set of images.\n",
        "        label_dir (str): Directory containing the labels.\n",
        "        transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        image_files (list): List of image filenames.\n",
        "        labels (list): List of labels corresponding to the images.\n",
        "    \"\"\"\n",
        "    def __init__(self, fused_dir, label_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Initializes the dataset by setting the image and label directories for three different sensors,\n",
        "        applying transformations if any, and loading image filenames and corresponding labels.\n",
        "\n",
        "        Args:\n",
        "            img1_dir (str): Path to the directory containing the first set of images.\n",
        "            img2_dir (str): Path to the directory containing the second set of images.\n",
        "            img3_dir (str): Path to the directory containing the third set of images.\n",
        "            label_dir (str): Path to the directory containing labels.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.img1_dir = img1_dir\n",
        "        self.img2_dir = img2_dir\n",
        "        self.img3_dir = img3_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = sorted(os.listdir(img1_dir))\n",
        "        self.labels = [self.load_label(os.path.join(label_dir, f\"{os.path.splitext(img)[0]}.txt\")) for img in self.image_files]\n",
        "\n",
        "    def load_label(self, idx):\n",
        "        \"\"\"\n",
        "        Loads the label for a given image.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the image.\n",
        "\n",
        "        Returns:\n",
        "            int: Label associated with the image.\n",
        "        \"\"\"\n",
        "        label_path = os.path.join(self.label_dir, f\"{idx}\")\n",
        "        with open(label_path, 'r') as file:\n",
        "            label = file.readline().strip()\n",
        "        return shape2number[label]\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the number of samples in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: Number of samples in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns a sample from the dataset.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the sample.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing the combined image and its label.\n",
        "        \"\"\"\n",
        "        img1_path = os.path.join(self.img1_dir, self.image_files[idx])\n",
        "        img2_path = os.path.join(self.img2_dir, self.image_files[idx])\n",
        "        img3_path = os.path.join(self.img3_dir, self.image_files[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        img1 = Image.open(img1_path)\n",
        "        img2 = Image.open(img2_path)\n",
        "        img3 = Image.open(img3_path)\n",
        "\n",
        "        if self.transform:\n",
        "\n",
        "            img1 = self.transform(img1)\n",
        "            img2 = self.transform(img2)\n",
        "            img3 = self.transform(img3)\n",
        "\n",
        "        combined_img = torch.cat((img1, img2, img3), dim=0)\n",
        "        return combined_img, label\n",
        "\n"
      ],
      "metadata": {
        "id": "CcKQWbTBER5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define modified LeNet-5 model architecture"
      ],
      "metadata": {
        "id": "ks2DjLlg7sFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LeNet-5 model\n",
        "class LeNet5(nn.Module):\n",
        "    \"\"\"\n",
        "    A PyTorch implementation of the LeNet-5 architecture for image classification.\n",
        "\n",
        "    Attributes:\n",
        "        conv1 (nn.Conv2d): First convolutional layer with 6 output channels.\n",
        "        conv2 (nn.Conv2d): Second convolutional layer with 16 output channels.\n",
        "        fc1 (nn.Linear): First fully connected layer.\n",
        "        fc2 (nn.Linear): Second fully connected layer.\n",
        "        fc3 (nn.Linear): Third fully connected layer for output.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=4, input_channels=1):\n",
        "        \"\"\"\n",
        "        Initializes the LeNet-5 model.\n",
        "\n",
        "        Args:\n",
        "            num_classes (int): Number of output classes. Default is 4.\n",
        "            input_channels (int): Number of input channels. Default is 1 (grayscale images).\n",
        "        \"\"\"\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_channels, 6, kernel_size=5, stride=1, padding=2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)\n",
        "        self.fc1 = nn.Linear(16*6*6, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the model.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor after passing through the network.\n",
        "        \"\"\"\n",
        "        x = F.relu(self.conv1(x))  # First convolution + ReLU activation\n",
        "        x = F.max_pool2d(x, 2)     # Max pooling with 2x2 window\n",
        "        x = F.relu(self.conv2(x))  # Second convolution + ReLU activation\n",
        "        x = F.max_pool2d(x, 2)     # Max pooling with 2x2 window\n",
        "        x = x.view(-1, 16*6*6)     # Flatten the tensor\n",
        "        x = F.relu(self.fc1(x))    # First fully connected layer + ReLU activation\n",
        "        x = F.relu(self.fc2(x))    # Second fully connected layer + ReLU activation\n",
        "        x = self.fc3(x)            # Third fully connected layer (output layer)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "jTOoX3aIM2N4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training models 1,2,3 without fusion and model_fusion with low level Pixel based fusion"
      ],
      "metadata": {
        "id": "Ag0XHPKn2X7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train the model\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs=10):\n",
        "    \"\"\"\n",
        "    Trains the given model using the provided data loader, loss function, and optimizer.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to train.\n",
        "        train_loader (DataLoader): DataLoader for the training data.\n",
        "        criterion (nn.Module): Loss function.\n",
        "        optimizer (optim.Optimizer): Optimizer.\n",
        "        epochs (int): Number of epochs to train. Default is 10.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate_model(model, test_loader):\n",
        "    \"\"\"\n",
        "    Evaluates the given model using the provided test data loader.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to evaluate.\n",
        "        test_loader (DataLoader): DataLoader for the test data.\n",
        "\n",
        "    Returns:\n",
        "        float: Accuracy of the model on the test data.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "def save_model_weights(model, path):\n",
        "    torch.save(model.state_dict(), path)\n",
        "\n",
        "# Define the transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Create the datasets\n",
        "img1_dir = os.path.join(dataset_path, 'img1')\n",
        "img2_dir = os.path.join(dataset_path, 'img2')\n",
        "img3_dir = os.path.join(dataset_path, 'img3')\n",
        "fused_dir = os.path.join(dataset_path, 'fused_dataset')\n",
        "label_dir = os.path.join(dataset_path, 'label')\n",
        "\n",
        "dataset1 = SingleSensorDataset(img1_dir, label_dir, transform)\n",
        "dataset2 = SingleSensorDataset(img2_dir, label_dir, transform)\n",
        "dataset3 = SingleSensorDataset(img3_dir, label_dir, transform)\n",
        "fused_dataset = ImageFusionDataset(fused_dir, label_dir, transform)\n",
        "\n",
        "\n",
        "# Split the datasets into training and testing sets\n",
        "train_size = int(0.8 * len(dataset1))\n",
        "test_size = len(dataset1) - train_size\n",
        "\n",
        "train_dataset1, test_dataset1 = random_split(dataset1, [train_size, test_size])\n",
        "train_dataset2, test_dataset2 = random_split(dataset2, [train_size, test_size])\n",
        "train_dataset3, test_dataset3 = random_split(dataset3, [train_size, test_size])\n",
        "train_dataset_fused, test_dataset_fused = random_split(fused_dataset, [train_size, test_size])\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 40\n",
        "\n",
        "train_loader1 = DataLoader(train_dataset1, batch_size=batch_size, shuffle=True)\n",
        "test_loader1 = DataLoader(test_dataset1, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "train_loader2 = DataLoader(train_dataset2, batch_size=batch_size, shuffle=True)\n",
        "test_loader2 = DataLoader(test_dataset2, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "train_loader3 = DataLoader(train_dataset3, batch_size=batch_size, shuffle=True)\n",
        "test_loader3 = DataLoader(test_dataset3, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "train_loader_fused = DataLoader(train_dataset_fused, batch_size=batch_size, shuffle=True)\n",
        "test_loader_fused = DataLoader(test_dataset_fused, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Instantiate models for each sensor dataset and fusion dataset\n",
        "model1 = LeNet5(num_classes=4)\n",
        "model2 = LeNet5(num_classes=4)\n",
        "model3 = LeNet5(num_classes=4)\n",
        "model_fusion = LeNet5(num_classes=4, input_channels=3)\n",
        "\n",
        "# Define loss function and optimizer with learning rate 0.001\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer1 = optim.Adam(model1.parameters(), lr=0.001)\n",
        "optimizer2 = optim.Adam(model2.parameters(), lr=0.001)\n",
        "optimizer3 = optim.Adam(model3.parameters(), lr=0.001)\n",
        "optimizer_fusion = optim.Adam(model_fusion.parameters(), lr=0.001)\n",
        "\n",
        "# Train the models\n",
        "print(\"Training model 1...\")\n",
        "train_model(model1, train_loader1, criterion, optimizer1, epochs=50)\n",
        "print(\"Training model 2...\")\n",
        "train_model(model2, train_loader2, criterion, optimizer2, epochs=50)\n",
        "print(\"Training model 3...\")\n",
        "train_model(model3, train_loader3, criterion, optimizer3, epochs=50)\n",
        "print(\"Training fused model...\")\n",
        "train_model(model_fusion, train_loader_fused, criterion, optimizer_fusion, epochs=50)\n",
        "\n",
        "# Save model weights\n",
        "save_model_weights(model1, '/content/drive/MyDrive/SF2/Lenet_model1_weights.pth')\n",
        "save_model_weights(model2, '/content/drive/MyDrive/SF2/Lenet_model2_weights.pth')\n",
        "save_model_weights(model3, '/content/drive/MyDrive/SF2/Lenet_model3_weights.pth')\n",
        "save_model_weights(model_fusion, '/content/drive/MyDrive/SF2/Lenet_fusion_model_weights.pth')\n"
      ],
      "metadata": {
        "id": "dqtJgcEG15nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# High Level Fusion- Majority Voting"
      ],
      "metadata": {
        "id": "fz5SINgu3-Sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform majority voting with tie breaker\n",
        "def majority_voting(models, test_loader):\n",
        "    \"\"\"\n",
        "    Performs majority voting on the predictions of multiple models with a tie-breaking mechanism.\n",
        "\n",
        "    Args:\n",
        "        models (list): List of trained models [model1, model2, model3, model_fusion].\n",
        "        test_loader (DataLoader): DataLoader for the test data.\n",
        "\n",
        "    Returns:\n",
        "        float: Accuracy of the majority voting method on the test data.\n",
        "        list: List of predictions made by the majority voting method.\n",
        "    \"\"\"\n",
        "    model1, model2, model3, model_fusion = models\n",
        "    model1.eval()\n",
        "    model2.eval()\n",
        "    model3.eval()\n",
        "    model_fusion.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    predictions_all = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            batch_size = images.size(0)\n",
        "            predictions = torch.zeros(batch_size, dtype=torch.long)\n",
        "            image1 = images[:, 0:1, :, :]\n",
        "            image2 = images[:, 1:2, :, :]\n",
        "            image3 = images[:, 2:3, :, :]\n",
        "\n",
        "            output1 = model1(image1)\n",
        "            output2 = model2(image2)\n",
        "            output3 = model3(image3)\n",
        "            output_fusion = model_fusion(images)\n",
        "\n",
        "            _, predicted1 = torch.max(output1, 1)\n",
        "            _, predicted2 = torch.max(output2, 1)\n",
        "            _, predicted3 = torch.max(output3, 1)\n",
        "            _, predicted_fusion = torch.max(output_fusion, 1)\n",
        "\n",
        "            stacked_preds = torch.stack((predicted1, predicted2, predicted3), dim=1)\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                vote_counts = torch.bincount(stacked_preds[i])\n",
        "                max_votes = torch.max(vote_counts)\n",
        "                candidates = torch.where(vote_counts == max_votes)[0]\n",
        "                if len(candidates) > 1:\n",
        "                    predictions[i] = predicted_fusion[i]  # Prioritize fused classifier in case of a tie\n",
        "                else:\n",
        "                    predictions[i] = candidates[0]\n",
        "\n",
        "            predictions_all.extend(predictions.tolist())\n",
        "\n",
        "            total += batch_size\n",
        "            correct += (predictions == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy, predictions_all\n",
        "\n",
        "# List of your already trained models\n",
        "models = [model1, model2, model3, model_fusion]"
      ],
      "metadata": {
        "id": "5X4v58RTfCx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluate Models and Calculate Accuracy"
      ],
      "metadata": {
        "id": "xoADwJJA4lot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the models\n",
        "accuracy1 = evaluate_model(model1, test_loader1)\n",
        "accuracy2 = evaluate_model(model2, test_loader2)\n",
        "accuracy3 = evaluate_model(model3, test_loader3)\n",
        "accuracy_fusion = evaluate_model(model_fusion, test_loader_fused)\n",
        "\n",
        "# Perform majority voting\n",
        "accuracy_majority_voting_fusion, _ = majority_voting(models, test_loader_fused)\n",
        "\n",
        "\n",
        "print('Accuracy of model 1:', accuracy1)\n",
        "print('Accuracy of model 2:', accuracy2)\n",
        "print('Accuracy of model 3:', accuracy3)\n",
        "print('Accuracy with low-level fusion:', accuracy_fusion)\n",
        "print('Accuracy with low and high level image fusion:', accuracy_majority_voting_fusion)\n"
      ],
      "metadata": {
        "id": "a1xWYep03e-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting Results with Confusion Matrices"
      ],
      "metadata": {
        "id": "1PUxIt8P5MOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictions(model, loader):\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  true_labels = []\n",
        "  with torch.no_grad():\n",
        "      for data, labels in loader:\n",
        "          outputs = model(data)\n",
        "          _, preds = torch.max(outputs, 1)\n",
        "          predictions.extend(preds.cpu().numpy())\n",
        "          true_labels.extend(labels.cpu().numpy())\n",
        "  return predictions, true_labels\n",
        "\n",
        "# Plot confusion matrices for each model\n",
        "def plot_confusion_matrix(true_labels, predictions, classes, title):\n",
        "  \"\"\"\n",
        "    Plot a confusion matrix for the given true labels and predictions.\n",
        "\n",
        "    Args:\n",
        "        true_labels (list): True labels.\n",
        "        predictions (list): Model predictions.\n",
        "        classes (list): List of class names.\n",
        "        title (str): Title for the plot.\n",
        "    \"\"\"\n",
        "  cm = confusion_matrix(true_labels, predictions)\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
        "  disp.plot(cmap=plt.cm.Blues)\n",
        "  plt.title(title)\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.ylabel('True label')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "class_labels = ['Circle', 'Square', 'Triangle', 'Pentagon']\n",
        "\n",
        "# Get predictions and true labels for each model\n",
        "predictions_model1, true_labels_model1 = get_predictions(model1, test_loader1)\n",
        "predictions_model2, true_labels_model2 = get_predictions(model2, test_loader2)\n",
        "predictions_model3, true_labels_model3 = get_predictions(model3, test_loader3)\n",
        "predictions_fusion, true_labels_fusion = get_predictions(model_fusion, test_loader_fused)\n",
        "accuracy_majority_voting_fusion, predictions_all = majority_voting(models, test_loader_fused) # Assign output of majority_voting to predictions_all\n",
        "\n",
        "# Calculate accuracy for each model\n",
        "accuracy_model1 = accuracy_score(true_labels_model1, predictions_model1)\n",
        "accuracy_model2 = accuracy_score(true_labels_model2, predictions_model2)\n",
        "accuracy_model3 = accuracy_score(true_labels_model3, predictions_model3)\n",
        "accuracy_fusion = accuracy_score(true_labels_fusion, predictions_fusion)\n",
        "\n",
        "# Plot Confusion Matrix for each model\n",
        "plot_confusion_matrix(true_labels_model1, predictions_model1, class_labels, \"Model 1\")\n",
        "plot_confusion_matrix(true_labels_model2, predictions_model2, class_labels, \"Model 2\")\n",
        "plot_confusion_matrix(true_labels_model3, predictions_model3, class_labels, \"Model 3\")\n",
        "plot_confusion_matrix(true_labels_fusion, predictions_all, class_labels, \"Model Fusion\")\n",
        "\n"
      ],
      "metadata": {
        "id": "MMHCZA0MQiA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VKenMWnpP4-j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}